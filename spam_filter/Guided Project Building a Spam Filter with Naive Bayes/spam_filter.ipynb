{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Filter using  multinomial Naive Bayes algorithm\n",
    "\n",
    "We'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). You can also download the dataset directly from this [link](https://dq-content.s3.amazonaws.com/433/SMSSpamCollection). The data collection process is described in more details on this [page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers.\n",
    "\n",
    "Solutions to this project can be found at this [link](https://github.com/dataquestio/solutions/blob/master/Mission433Solutions.ipynb) or by clicking the key icon at the top right of the interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original data\n",
    "We import the data and inspect the format and quality of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df_raw =  pd.read_csv('SMSSpamCollection', sep='\\t', header=None)\n",
    "sms_df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to represent correctly the attributes\n",
    "sms_df = sms_df_raw.copy()\n",
    "dic_col = {0:'Label', 1:'SMS'}\n",
    "sms_df.rename(columns=dic_col,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head()\n",
    "sms_df.loc[:,'Label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems alright, with 5572 sms as it was indicated in the daata source. The SMS message, however, seems rather cryptic. The label options are either 'ham' or 'spam'. The first indicates that is a real SMS, whereas the latter is a SPAM mesage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.loc[:,'Label'].value_counts()/sms_df.loc[:,'Label'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a 86% of the SMS are 'ham' vs only 13% 'spam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup for algorithm\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n",
    "\n",
    "* The training set will have 4,458 messages (about 80% of the dataset).\n",
    "* The test set will have 1,114 messages (about 20% of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df = sms_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "sms_train = sms_df.iloc[:4458,:]\n",
    "sms_test = sms_df.iloc[4458:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam percentage in train is 13.458950201884253% and in test 13.195691202872531%\n"
     ]
    }
   ],
   "source": [
    "spam_perc_train = sms_train.loc[:,'Label'].value_counts()[1]/sms_train.loc[:,'Label'].value_counts().sum()\n",
    "spam_perc_test = sms_test.loc[:,'Label'].value_counts()[1]/sms_test.loc[:,'Label'].value_counts().sum()\n",
    "print('Spam percentage in train is {0}% and in test {1}%'.format(spam_perc_train*100, spam_perc_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentages in both seem fairly close. This means that the test sample should be large enough for our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "We proceed to do the following operations:\n",
    "* Remove punctuation or non-word characters\n",
    "* Transform the text into lower case\n",
    "* Transform the data from Label, SMS -> Label, w1_counts, w2_counts, ..., where w1_coutns means the appearaences of word w1 from the dictionary of the data set in the particular SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df['SMS'] = sms_df['SMS'].str.replace(r'\\W', ' ').str.replace(r'\\s+',' ').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes princess are you going to make me moan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                          SMS\n",
       "0   ham                  yep by the pretty sculpture\n",
       "1   ham  yes princess are you going to make me moan "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the vocabulary of our training dataset\n",
    "vocabulary = []\n",
    "for index in sms_train.index:\n",
    "    words = sms_train.loc[index,'SMS'].split()\n",
    "    vocabulary = vocabulary + words\n",
    "vocabulary = list(set(vocabulary))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data frame into a new format, such that each word in the vocabulary is a column, \n",
    "# and its value are the counts of that word in the SMS message\n",
    "# we create new data frame starting from a dictionary\n",
    "word_counts_per_sms = {unique_word: [0] * len(sms_train['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "def get_new_sms_df_format(df, word_counts_per_sms):\n",
    "    for index, sms in enumerate(df['SMS']):\n",
    "        for word in sms.split():\n",
    "            if word in word_counts_per_sms.keys():\n",
    "                word_counts_per_sms[word][index] += 1\n",
    "    # add the Label column to the data\n",
    "    new_df = pd.DataFrame(word_counts_per_sms)\n",
    "    new_df = pd.concat([df, new_df], axis=1)\n",
    "    return new_df\n",
    "    \n",
    "training_set = get_new_sms_df_format(sms_train, word_counts_per_sms)\n",
    "test_set = get_new_sms_df_format(sms_test, word_counts_per_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the algorithm\n",
    "Here we will do the following:\n",
    "* Calculate the spam and ham probability in the training set\n",
    "* Calculate the total number of words in spam and ham sets\n",
    "* Build the partial word probabilities for the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: (p_spam + p_ham) = 1.0\n"
     ]
    }
   ],
   "source": [
    "is_spam = (training_set['Label'] == 'spam')\n",
    "is_ham = (training_set['Label'] == 'ham')\n",
    "p_spam = is_spam.sum()/training_set.shape[0]\n",
    "p_ham = is_ham.sum()/training_set.shape[0]\n",
    "print('Sanity check: (p_spam + p_ham) = {0}'.format(p_spam+p_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_w_spam = training_set[is_spam].iloc[:,2:].sum().sum()\n",
    "n_w_ham = training_set[is_ham].iloc[:,2:].sum().sum()\n",
    "n_w_vocabulary = len(vocabulary)\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters for algorithm\n",
    "* Build the partial probabilities of the words P(word|ham), P(word|spam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a probability dictionary for each word in the \n",
    "# dictionary using the training dataset\n",
    "def get_word_probability_subset(wordcount_df, vocabulary, \n",
    "                                n_w_subset, alpha):\n",
    "    p_words = {word:0 for word in vocabulary}\n",
    "    n_w_vocabulary = len(vocabulary)\n",
    "    \n",
    "    counts_per_word = wordcount_df[vocabulary].sum(axis=0)\n",
    "    for word in vocabulary:\n",
    "        p_words[word] = (counts_per_word[word] + alpha)/(n_w_subset + \n",
    "                                                        alpha*n_w_vocabulary)\n",
    "    return p_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_word_spam = get_word_probability_subset(training_set[is_spam], vocabulary, n_w_spam, alpha)\n",
    "p_word_ham = get_word_probability_subset(training_set[is_ham], vocabulary, n_w_ham, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the message as \"spam\" or \"ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message, p_spam, p_ham, p_word_spam, p_word_ham):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    vocabulary = p_word_spam.keys()\n",
    "    for word in message:\n",
    "        if word in vocabulary:\n",
    "            p_spam_given_message *= p_word_spam[word]\n",
    "            p_ham_given_message *= p_word_ham[word]\n",
    "\n",
    "    #print('P(Spam|message):', p_spam_given_message)\n",
    "    #print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        #print('Label: Ham')\n",
    "        prediction = 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        #print('Label: Spam')\n",
    "        prediction = 'spam'\n",
    "    else:\n",
    "        #print('Equal proabilities, have a human classify this!')\n",
    "        prediction = 'spam'\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message1: spam, message2: ham\n"
     ]
    }
   ],
   "source": [
    "message1 = 'WINNER!! This is the secret code to unlock the money: C3421.'\n",
    "message2 = \"Sounds good, Tom, then see u there\"\n",
    "prediction1 = classify(message1, p_spam, p_ham, p_word_spam, p_word_ham)\n",
    "prediction2 = classify(message2, p_spam, p_ham, p_word_spam, p_word_ham)\n",
    "print('message1: {0}, message2: {1}'.format(prediction1, prediction2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test algorithm performance\n",
    "We test the accuracy of the algorithm with the test dataset\n",
    "We check the sensitivity and specifity of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "test_copy = sms_test.reset_index(drop=True)\n",
    "predictions = []\n",
    "for index, sms in enumerate(test_copy['SMS']):\n",
    "    prediction = classify(sms, p_spam, p_ham, p_word_spam, p_word_ham)\n",
    "    predictions.append(prediction)\n",
    "test_copy['prediction'] = predictions\n",
    "test_copy['correct'] = (test_copy['prediction']==test_copy['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9874326750448833, sensitivity: 0.9586206896551724, specifity: 0.9917440660474717\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_copy['correct'].sum()/test_copy.shape[0]\n",
    "sensitivity = (test_copy.loc[test_copy['correct'],'prediction']=='spam').sum()/(test_copy['prediction']=='spam').sum()\n",
    "specifity = (test_copy.loc[test_copy['correct'],'prediction']=='ham').sum()/(test_copy['prediction']=='ham').sum()\n",
    "print('accuracy: {0}, sensitivity: {1}, specifity: {2}'.format(accuracy, sensitivity, specifity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As we see, this simple algorithm is capable of performing rather well, and achieves an accuracy of 98.74%, with a 95% sensitivity and 99% specifity. This is great, for 100 SPAM SMS only 5 will go over the filter, moreover, from 100 correct SMS only a ~1% will be mistaken for SPAM. This is great, otherwise we might miss some really important SMS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
