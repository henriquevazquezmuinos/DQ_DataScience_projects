{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker News Pipeline\n",
    "\n",
    "We will use the pipeline we have been building, and apply it to a real world data pipeline project. From a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us.\n",
    "The data we will use comes from a Hacker News (HN) API that returns JSON data of the [top stories in 2014](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON).\n",
    "\n",
    "We have already downloaded a list of JSON posts to a file called hn_stories_2014.json. The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "* created_at: A timestamp of the story's creation time.\n",
    "* created_at_i: A unix epoch timestamp.\n",
    "* url: The URL of the story link.\n",
    "* objectID: The ID of the story.\n",
    "* author: The story's author (username on HN).\n",
    "* points: The number of upvotes the story had.\n",
    "* title: The headline of the post.\n",
    "* num_comments: The number of a comments a post has.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "from pipeline import Pipeline, build_csv\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# read json hn_stories_2014.json and return the data as a list\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    filename = 'hn_stories_2014.json'\n",
    "    with open(filename, 'r') as f:\n",
    "        hn_dict = json.load(f)\n",
    "    return hn_dict['stories']\n",
    "\n",
    "# fildter stories only with more than 50 points and 1 comment\n",
    "# we also discard Ask HN comments\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):            \n",
    "    def inner():\n",
    "        for story in stories:\n",
    "            not_ask = story['title'].find('Ask HN ') == -1\n",
    "            if story['points'] > 50 and story['num_comments'] and not_ask:\n",
    "                yield story\n",
    "        \n",
    "    return inner\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(filtered_stories):\n",
    "    def parsed_func():\n",
    "        gen = filtered_stories()\n",
    "        fields =[\n",
    "            'objectID', 'created_at', \n",
    "            'url', 'points', 'title'\n",
    "                ]\n",
    "        for row in gen:\n",
    "            parsed = []\n",
    "            for fld in fields:\n",
    "                fld_str = str(row[fld])\n",
    "                if fld=='created_at':\n",
    "                    fld_str = str(parse(fld_str))\n",
    "                parsed.append(fld_str)        \n",
    "            yield parsed\n",
    "\n",
    "    parse_gen = parsed_func()\n",
    "    csv_file = build_csv(\n",
    "        parse_gen,\n",
    "        #header=fields,\n",
    "        # Using file-like object instead of `temporary.csv`.            \n",
    "        file=io.StringIO()\n",
    "    )\n",
    "    return csv_file\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    def inner():\n",
    "        csv_rows = csv.reader(csv_file, delimiter=',')\n",
    "        header = next(csv_rows)\n",
    "        for row in csv_rows:\n",
    "            yield row[4]\n",
    "    return inner\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(title_gen):\n",
    "    def inner():\n",
    "        for title in title_gen():\n",
    "            title = re.sub(r\"[^\\w\\d\\s]\", \"\", title).lower()\n",
    "            title = re.sub(r\"\\s+\", r\" \", title)\n",
    "            yield title\n",
    "    return inner\n",
    "\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(cleaned_gen):\n",
    "    keyword_dict = {}\n",
    "    for title in cleaned_gen():\n",
    "        words = title.split()\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                if word in keyword_dict:\n",
    "                    keyword_dict[word] += 1outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = file_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = filter_stories(stories)()\n",
    "test = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = json_to_csv(filter_stories(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_gen = extract_titles(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gen = clean_titles(title_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: new, counts: 184\n",
      "word: google, counts: 166\n",
      "word: ask, counts: 124\n",
      "word: bitcoin, counts: 101\n",
      "word: open, counts: 96\n",
      "word: programming, counts: 91\n",
      "word: web, counts: 89\n",
      "word: data, counts: 87\n",
      "word: video, counts: 78\n",
      "word: python, counts: 76\n",
      "word: code, counts: 73\n",
      "word: facebook, counts: 71\n",
      "word: released, counts: 70\n",
      "word: using, counts: 69\n",
      "word: source, counts: 68\n",
      "word: free, counts: 65\n",
      "word: game, counts: 64\n",
      "word: javascript, counts: 64\n",
      "word: 2014, counts: 64\n",
      "word: 2013, counts: 64\n",
      "word: internet, counts: 62\n",
      "word: c, counts: 59\n",
      "word: linux, counts: 58\n",
      "word: microsoft, counts: 58\n",
      "word: work, counts: 58\n",
      "word: dont, counts: 57\n",
      "word: app, counts: 57\n",
      "word: pdf, counts: 54\n",
      "word: software, counts: 53\n",
      "word: language, counts: 53\n",
      "word: use, counts: 52\n",
      "word: startup, counts: 51\n",
      "word: make, counts: 49\n",
      "word: apple, counts: 49\n",
      "word: time, counts: 48\n",
      "word: security, counts: 47\n",
      "word: yc, counts: 47\n",
      "word: github, counts: 44\n",
      "word: nsa, counts: 44\n",
      "word: like, counts: 43\n",
      "word: windows, counts: 43\n",
      "word: way, counts: 41\n",
      "word: project, counts: 41\n",
      "word: 1, counts: 41\n",
      "word: heartbleed, counts: 40\n",
      "word: world, counts: 40\n",
      "word: computer, counts: 39\n",
      "word: developer, counts: 39\n",
      "word: users, counts: 39\n",
      "word: big, counts: 36\n",
      "word: ios, counts: 36\n",
      "word: design, counts: 36\n",
      "word: twitter, counts: 36\n",
      "word: git, counts: 36\n",
      "word: life, counts: 35\n",
      "word: vs, counts: 35\n",
      "word: online, counts: 35\n",
      "word: ceo, counts: 35\n",
      "word: os, counts: 35\n",
      "word: day, counts: 34\n",
      "word: best, counts: 33\n",
      "word: simple, counts: 33\n",
      "word: apps, counts: 33\n",
      "word: years, counts: 33\n",
      "word: android, counts: 33\n",
      "word: court, counts: 32\n",
      "word: mt, counts: 32\n",
      "word: browser, counts: 31\n",
      "word: says, counts: 31\n",
      "word: api, counts: 31\n",
      "word: site, counts: 31\n",
      "word: gox, counts: 31\n",
      "word: learning, counts: 31\n",
      "word: guide, counts: 31\n",
      "word: firefox, counts: 31\n",
      "word: engine, counts: 30\n",
      "word: mozilla, counts: 30\n",
      "word: problem, counts: 30\n",
      "word: amazon, counts: 30\n",
      "word: fast, counts: 30\n",
      "word: server, counts: 30\n",
      "word: text, counts: 29\n",
      "word: year, counts: 29\n",
      "word: better, counts: 29\n",
      "word: does, counts: 29\n",
      "word: introducing, counts: 29\n",
      "word: people, counts: 28\n",
      "word: money, counts: 28\n",
      "word: million, counts: 28\n",
      "word: built, counts: 28\n",
      "word: help, counts: 28\n",
      "word: tech, counts: 28\n",
      "word: stop, counts: 28\n",
      "word: support, counts: 28\n",
      "word: learn, counts: 27\n",
      "word: did, counts: 27\n",
      "word: development, counts: 27\n",
      "word: developers, counts: 27\n",
      "word: 3, counts: 27\n",
      "word: 2048, counts: 26\n"
     ]
    }
   ],
   "source": [
    "sorted_keywords = sorted(outputs[build_keyword_dictionary].items(), key=lambda x: x[1])\n",
    "top100 = sorted_keywords[-100:][::-1]\n",
    "for word, counts in top100:\n",
    "    print('word: '+word+', counts: '+str(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The results seem coherent, the most common words are new, google, ask, programming, bitcoin. All seem very common words in the field of programming. Our pipeline seems to work fine! We see that the simplicity and versatility of the pipeline allows to perform streamlined with little effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
